# -*- coding: utf-8 -*-
"""Sentiment Analysis and Forecasting of Amazon Reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UF9anPGOhUB800OoHc0zv40YiHBZ4_RG

**Install required Packages**
"""

pip install pandas matplotlib scikit-learn

"""**Data set Loading**"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/Reviews.csv')  # Update with your file path

# Display basic information about the dataset
print("Data before cleaning:")
print(df.info())

# Check the first few rows of the dataset
print(df.head())

"""**Cleaning of Data**"""

# Drop rows with missing values in critical columns
df = df.dropna(subset=['Score', 'Time'])

# Convert the 'Time' column from int to datetime format (assuming it's in Unix timestamp format)
df['Date'] = pd.to_datetime(df['Time'], unit='s')

# Drop the 'Time' column as we now have 'Date'
df.drop(columns=['Time'], inplace=True)

# Display cleaned data
print("Data after cleaning:")
print(df.info())

"""**SQL Integration for Data Exploration**"""

import sqlite3

# Create a connection to an in-memory SQLite database
conn = sqlite3.connect(':memory:')

# Write the pandas DataFrame to the SQL database as a table
df.to_sql('reviews', conn, if_exists='replace', index=False)

# Confirm the data was written to the database by running a simple SQL query
#Example Query 1
query = "SELECT * FROM reviews LIMIT 5"
result = pd.read_sql(query, conn)
print("Data in SQL (first 5 rows):")
print(result)

# SQL query to calculate average score
query = "SELECT AVG(Score) AS Average_Score FROM reviews"
average_score = pd.read_sql(query, conn)
print("Average Review Score:")
print(average_score)

# SQL query to find the top 5 most helpful reviews
query = """
SELECT ProfileName, Score, HelpfulnessNumerator, HelpfulnessDenominator
FROM reviews
ORDER BY HelpfulnessNumerator DESC
LIMIT 5
"""
most_helpful_reviews = pd.read_sql(query, conn)
print("Most Helpful Reviews:")
print(most_helpful_reviews)

"""**Visualizing Review Scores Distribution**"""

import matplotlib.pyplot as plt

# Plotting the distribution of scores
plt.figure(figsize=(10, 6))
df['Score'].value_counts().sort_index().plot(kind='bar', color='skyblue')
plt.title('Distribution of Review Scores')
plt.xlabel('Scores')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# Close the SQLite connection
conn.close()

"""**Sentiment Analysis Using VADER**"""

import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()

# Add sentiment score to the DataFrame
df['Sentiment'] = df['Text'].apply(lambda x: sia.polarity_scores(x)['compound'])
print("Data with Sentiment Scores:")
print(df[['Text', 'Sentiment']].head())

"""**Anomaly Detection Using Z-Scores**"""

from scipy import stats

# Calculate Z-scores
df['Z_Score'] = stats.zscore(df['Sentiment'])
anomalies = df[(df['Z_Score'] > 3) | (df['Z_Score'] < -3)]
print("Anomalies in Sentiment Scores:")
print(anomalies[['Text', 'Sentiment', 'Z_Score']])

"""**Time Series Forecasting Using ARIMA**"""

# Group by Date
daily_sentiment = df.groupby('Date')['Sentiment'].mean().reset_index()

# Time Series Forecasting (simple example)
from statsmodels.tsa.arima.model import ARIMA

model = ARIMA(daily_sentiment['Sentiment'], order=(1, 1, 1))
model_fit = model.fit()
forecast = model_fit.forecast(steps=10)
print("Forecasted Sentiment:")
print(forecast)

"""**Predictive Modeling Using Random Forest**"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

X = daily_sentiment[['Date']]
y = daily_sentiment['Sentiment']

# Define model and parameters
rf = RandomForestRegressor()
params = {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}
grid_search = GridSearchCV(rf, params, cv=3)

# Fit model
grid_search.fit(X, y)
print("Best Hyperparameters:")
print(grid_search.best_params_)

"""**Visualization Using Plotly**"""

import plotly.express as px

# Create interactive scatter plot
fig = px.scatter(df, x='Date', y='Sentiment', title='Sentiment Over Time')
fig.show()

"""**Model Interpretability Using SHAP**"""

import shap

explainer = shap.Explainer(grid_search.best_estimator_)
shap_values = explainer(X)

# Plot SHAP values
shap.summary_plot(shap_values, X)

"""**Save Random Forest Model**"""

import joblib

# Save the trained Random Forest model
joblib.dump(grid_search.best_estimator_, 'sentiment_rf_model.pkl')

model = joblib.load('sentiment_rf_model.pkl')

# Save the processed DataFrame with sentiment scores
df.to_csv('processed_reviews.csv', index=False)

"""**Save the DataFrame**"""

import pandas as pd

# Load the saved processed DataFrame
df = pd.read_csv('processed_reviews.csv')

"""**Downlaod the Model**"""

from google.colab import files

# Download the Random Forest model
files.download('sentiment_rf_model.pkl')

# Download the processed CSV
files.download('processed_reviews.csv')

"""**Get the model in drive**"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Save the model and DataFrame to Google Drive
df.to_csv('/content/drive/MyDrive/processed_reviews.csv', index=False)
joblib.dump(grid_search.best_estimator_, '/content/drive/MyDrive/sentiment_rf_model.pkl')